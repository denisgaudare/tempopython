# ðŸ§  Exercices avancÃ©s PyArrow

---

## ðŸ§ª Exercice 1 â€” CrÃ©ation et typage explicite

**Objectif** : CrÃ©er un tableau Arrow typÃ© explicitement pour un gros DataFrame (> 1M lignes).

### Code Ã  complÃ©ter :

```python
import pyarrow as pa
import numpy as np

n = 1_000_000
data = {
    "user_id": np.arange(n),
    "age": np.random.randint(18, 90, size=n),
    "score": np.random.rand(n),
    "active": np.random.choice([True, False], size=n)
}

# CrÃ©er un tableau Arrow avec un schÃ©ma typÃ©
schema = ...
table = pa.table(data, schema=schema)
print(table.schema)
```

ðŸ§  *Erreur frÃ©quente : oublier de convertir les types numpy â†’ Arrow types compatibles.*

---

## ðŸ§ª Exercice 2 â€” Ã‰criture Parquet optimisÃ©e

**Objectif** : Ã‰crire un tableau en Parquet avec **compression**, puis le relire et vÃ©rifier sa taille.

### Code Ã  complÃ©ter :

```python
import pyarrow.parquet as pq
import os

# table = ... (gÃ©nÃ©rÃ© dans lâ€™exercice 1)
output_path = "big_data.parquet"

# Ã‰criture avec compression zstd
...

# Relecture + taille fichier
table2 = ...
print(table2.num_rows, os.path.getsize(output_path) / 1024**2, "MB")
```

ðŸ§  *Erreur frÃ©quente : mauvaise compression ou oubli de `use_dictionary=True`.*

---

## ðŸ§ª Exercice 3 â€” Lecture conditionnelle avec filtre

**Objectif** : Lire un fichier Parquet et filtrer les donnÃ©es sans tout charger.

### Code Ã  complÃ©ter :

```python
# Lire uniquement les utilisateurs actifs avec age > 50
filtered = pq.read_table(
    "big_data.parquet",
    filters=[
        ("active", "==", True),
        ("age", ">", 50)
    ]
)

print(filtered.num_rows)
```

ðŸ§  *Erreur frÃ©quente : vouloir filtrer manuellement avec `.to_pandas()` (inefficace en mÃ©moire).*

---

## ðŸ§ª Exercice 4 â€” AgrÃ©gation sans conversion pandas

**Objectif** : Calculer la moyenne de score par tranche d'Ã¢ge sans passer par Pandas.

### Code Ã  complÃ©ter :

```python
# Grouper par tranches : 18â€“29, 30â€“49, 50â€“69, 70+
# Bonus : utiliser pyarrow.compute (pa.compute)

import pyarrow.compute as pc

# Ã‰tapes :
# 1. ajouter une colonne 'age_group'
# 2. grouper et agrÃ©ger

# -> Cette partie nÃ©cessite de faire des conditions + concatÃ©nation dans Arrow
```

ðŸ§  *C'est un peu tricky, car `pyarrow` nâ€™a pas de `groupby()` direct comme Pandas.*

---

## âœ… CorrigÃ©s

---

### âœ… CorrigÃ© 1

```python
schema = pa.schema([
    ("user_id", pa.int32()),
    ("age", pa.int8()),
    ("score", pa.float64()),
    ("active", pa.bool_())
])

table = pa.table(data, schema=schema)
```

---

### âœ… CorrigÃ© 2

```python
pq.write_table(table, output_path, compression="zstd", use_dictionary=True)
table2 = pq.read_table(output_path)
```

---

### âœ… CorrigÃ© 3

```python
filtered = pq.read_table(
    output_path,
    filters=[("active", "==", True), ("age", ">", 50)]
)
```

---

### âœ… CorrigÃ© 4 (simplifiÃ©)

```python
import pyarrow.compute as pc

age = table.column("age")
conditions = [
    pc.and_(age >= 18, age <= 29),
    pc.and_(age >= 30, age <= 49),
    pc.and_(age >= 50, age <= 69),
    age >= 70
]
labels = ["18-29", "30-49", "50-69", "70+"]

age_groups = pc.case_when(zip(conditions, [pa.scalar(l) for l in labels]), "70+")
table = table.append_column("age_group", age_groups)

# Simuler un groupby avec Pandas :
df = table.to_pandas()
result = df.groupby("age_group")["score"].mean()
print(result)
```

