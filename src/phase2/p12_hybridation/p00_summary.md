# Tirer parti √† la fois du **CPU** et de l‚Äô**I/O** efficacement en Python !
## Combiner **multiprocessing** (pour le calcul) et **asyncio** (pour les I/O). 

### üß† Strat√©gie g√©n√©rale :

- **Parties CPU-intensives** ‚Üí utilisent plusieurs **processus** (`multiprocessing`)
- **Parties I/O-intensives** (ex : requ√™tes web, lecture fichiers r√©seau) ‚Üí utilisent **asyncio**

---

### ‚úÖ Exemple concret :

Imaginons un script qui :
1. Charge des fichiers JSON distants (I/O)
2. Fait un gros traitement math√©matique sur chaque (CPU)
3. Sauvegarde les r√©sultats (I/O)

---

### üì¶ Code hybride `asyncio` + `multiprocessing`

```python
import asyncio
import aiohttp
import json
from multiprocessing import Pool, cpu_count

# === Partie CPU (calcul intensif) ===

def analyse_json(data):
    # Simulation de calcul lourd
    total = sum(x**2 for x in data["values"])
    return {"id": data["id"], "score": total}

# === Partie I/O (chargement fichiers) ===

async def fetch_json(session, url):
    async with session.get(url) as response:
        return await response.json()

async def download_all(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_json(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# === Orchestration g√©n√©rale ===

def main():
    urls = [
        "https://example.com/file1.json",
        "https://example.com/file2.json",
        "https://example.com/file3.json",
    ]

    # √âtape 1 : Download des fichiers en parall√®le (I/O avec asyncio)
    json_data = asyncio.run(download_all(urls))

    # √âtape 2 : Traitement CPU en parall√®le (multiprocessing)
    with Pool(processes=cpu_count()) as pool:
        results = pool.map(analyse_json, json_data)

    # √âtape 3 : Sauvegarde (I/O, ici simul√©e)
    for result in results:
        with open(f"output_{result['id']}.json", "w") as f:
            json.dump(result, f)

if __name__ == "__main__":
    main()
```

---

### üí° Points cl√©s :

- `asyncio.run(...)` pour lancer le code asynchrone (I/O)
- `Pool.map(...)` pour parall√©liser le calcul CPU
- On **√©vite de m√©langer** asyncio et multiprocessing dans le m√™me thread/process
- Les donn√©es JSON sont d‚Äôabord collect√©es, puis trait√©es

---

### üîß En bonus : alternatives avanc√©es

- Combiner avec **ThreadPoolExecutor** pour du I/O bloquant (genre acc√®s fichiers, DB)
- D√©couper le pipeline avec un **scheduler** (ex : `concurrent.futures`, `ray`, ou `dask`)

---
